{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56cb1ecf",
   "metadata": {},
   "source": [
    "# Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3ab08",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<li><a href='#intro'>Introduction</a></li>\n",
    "<li><a href='#gathering'>Data Gathering</a></li>\n",
    "<li><a href='#assess'> Assessing Data</a></li>\n",
    "<li><a href='#cleaning'>Cleaning Data</a></li>\n",
    "<li><a href='#conclusion'>Conclusion</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e4ad7d",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "### Introduction\n",
    ">The data for this project is WeRateDogs Twitter data. The data was gathered from three sources. Two from Udacity servers and one from Twitter API.\n",
    "\n",
    "##### Sources\n",
    ">`twitter-archive-enhanced.csv` and `image-prediction.tsv` was downloaded from udacity Data Analyst Nanodegree course project page.\n",
    "\n",
    ">`tweet-json.txt` is a text file holding the data retrieved from Twitter using the Twitter API.\n",
    "\n",
    "###### Data Description\n",
    ">`twitter-archive-enhanced.csv` contains data about the tweet id, the time the post was tweeted 'timestamp', the source of the tweet, the name of the dog, the rating and the dog stages\n",
    "\n",
    ">`image-prediction.tsv` contains three different predictions of the dog breeds each with their level of confidence\n",
    "\n",
    ">`tweet-json.txt` contains an additional data of the tweets retweet counts and favorite counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba962f",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "\n",
    "### Data Gathering\n",
    "\n",
    ">`twitter-archive-enhanced.csv`  was downloaded manually from <a href=https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv> DAND-project page</a>  and loaded into pandas\n",
    "\n",
    ">`image-prediction.tsv` was also downloaded from Udacity servers using request library and loaded to pandas\n",
    "\n",
    ">`tweet-json.txt` was gathered from Twitter using the Twitter API by requesting  access tokens from the Twitter's developer page  to get access to use the Twitte API. Using tweepy library, the tweets was retrieved for each id into a .txt file and later retrieve  retweet_count and favorite_count using the JSON library in python and loaded into pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37149cb",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "### Assessing Data\n",
    "\n",
    "#### Quality Issues\n",
    "\n",
    "#### _`twitter-archive-enhanced table`_\n",
    "Through visual assessment, it was noted that some columns do not have the correct data types and also some values of some columns are not invalid or are not properly recorded and with some others missing.\n",
    "\n",
    "> To start with, the `tweet_id` column has int as a data type instead of string. The id of each tweet is to serve as a unique identification of tweets and will not serve any other purpose in terms of computation. Thus, to prevent any unforseen analysis error, it should be converted into strings.\n",
    "\n",
    "> Also, `timestamp, retweeted_status_timestamp` are columns containing timestamp of each tweet. The timestamp should be converted to pandas datetime dtype for easy time series analysis. The timestamp is an object data type with '+0000' at the end.\n",
    "\n",
    "> Further, `doggo, floofer, pupper, puppo` columns have limited values thus should be converted to category dtype instead of its current string dtype. Also, the Nan values are recorded as a string 'None' and moreover, the the four stages were recorded in four separate column\n",
    "\n",
    "> The `name` column through visual assessment, it could be seen that the names of some dogs are verbs, articles etc like 'a', 'such', 'the' and more. And also, NaN values are recorded as NoNe.\n",
    "\n",
    "> `source`  column values contains html tags and some other text along with the source url. Thus, the values appears to have not being properly recorded.\n",
    "\n",
    "#### _`image_prediction`_  table\n",
    "\n",
    "> With the image prediction table, it has a total record of 2075 which appears to be incomplete as compared to `twitter-archive-enhance` table and the `additional_tweet` table.\n",
    "> The table also has its tweet_id column dtype being int instead of string and so as the `additional_tweet` table\n",
    "\n",
    "#### Tidiness Issues\n",
    "\n",
    "> The `image_prediction` table columns, p1,p1_conf,p2,p2_conf,p3,p3_conf, jpg_url are less descriptive thus renaming them into a more descriptive names will make he table more tidy.\n",
    ">Also, the image prediction table has three different predicitions for the type of breed a dog is with each having a level of confidence for its prediction. Through both programatic and visual assessment, the confidence interval for the #1 prediction has a higher level f confidence as compared to the rest. Thus will be more accurate to use the #1 prediction to determine the dog breed from each tweeted image thus rendering the other prediction less usedful.\n",
    "\n",
    "> To normalise the values of `p1,p2,p3` column values, values with their first letter in lower case should be made uppercase\n",
    "\n",
    "> To make analysis easier, the tidied `image_prediction` and `additional_tweet` table should be part of `twitter-archive-enhanced` table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a371bd",
   "metadata": {},
   "source": [
    "<a id= 'cleaning'></a>\n",
    "### Cleaning Data\n",
    "The three datasets were cleaned and saved into a single file called `twitter_archive_master.csv`. The datasets were cleaned for quality and tidiness issues. Though a thorough cleaning was done for the identified issues, there still remain some issues unidentified and identified that needs to be addressed if a very deep analysis should be carried out, however, the data has been wrangled enough to carry out a more accurate and reliable analysis.\n",
    "\n",
    "> All `tweet_id` columns in all the datasets were converted to an object dtype. Also,\n",
    "in `twitter-archive-enhanced table` the `timestamp` and `retweeted_status_timestamp` columns to converted to datetime dtype for easy analysis and prevent unforseen analysis errors.\n",
    "\n",
    "> Moreover, `doggo, floofer, pupper, puppo` in  `twitter-archive-enhanced table` columns values with None as values replace empty string values and the columns have been converted into 'category' dtype becuase of its limited values.Also, the four columns were combined into one column called stage. And `name` values that are invalid has been replaced with NaN values as we cannot replace it with its real value. Also, the `source` column values has been strip off the tags.\n",
    "\n",
    "> Furthermore, p1,p1_conf,p2,p2_conf,p3,p3_conf, jpg_url, the `image_prediction` table column was replace with prediction_1, prediction_1_conf,prediction_2, prediction_2_conf,prediction_3, prediction_3_conf,image_url respectively and the values of p1,p2,p3 normalised. However, because prediction_1 has the highest level of confidence as compared to the other predictions, we kept the `tweet_id, image_url, prediction_1` columns.Note that the other predictions and their confidence were not dropped because it made the data untidy but it was dropped because it will not be used in the analysis. \n",
    "\n",
    "> Because the size of the image_prediction table is 2075 in contrast to 2356 size in `twitter-archive-enhanced table` and `additional_tweet` and since we are interested in tweets with images, the rows without image url in `image_prediction` table in the foremention tables was dropped and to have a complete dataframe for easy analysis, both `image_prediction and additional_tweet` was merged into  using the `tweet_id` column `twitter-archive-enhanced table` and the `prediction_1` column was renamed to `dog_breed` for clarification.\n",
    "\n",
    "> Lastly, the interest of the analysis is focused on original dog ratings, all columns with retweets i.e. retweeted_status_id\tretweeted_status_user_id\tretweeted_status_timestamp\trows with values were dropped and the column themselves were dropped later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40b69b",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "\n",
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3545e99",
   "metadata": {},
   "source": [
    "The wrangled data has been saved as `twitter_archive_master.csv`. Though the dataset has been wrangled enough to produce a more accurate result for analysis and most of the critical quality issues have been resolved and has been tidied up, there are still some wrangling that can be done to make the dataset more tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ff880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
